{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf7MBrBeaJK0",
        "outputId": "60be961f-6135-444e-c585-5ea82c851e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (486, 22)\n",
            "\n",
            "Columns: ['task_id', 'task_description', 'deadline', 'priority', 'assigned_user', 'user_workload', 'past_task_behavior', 'category', 'status', 'time_taken_(hours)', 'deadline_days_remaining', 'task_length', 'has_keyword_urgent', 'is_weekend_deadline', 'category_encoded', 'status_encoded', 'priority_encoded', 'is_completed', 'user_current_load', 'past_behavior_score', 'workload', 'deadline_days']\n",
            "\n",
            "Data types:\n",
            "task_id                     object\n",
            "task_description            object\n",
            "deadline                    object\n",
            "priority                    object\n",
            "assigned_user               object\n",
            "user_workload              float64\n",
            "past_task_behavior          object\n",
            "category                    object\n",
            "status                      object\n",
            "time_taken_(hours)         float64\n",
            "deadline_days_remaining    float64\n",
            "task_length                float64\n",
            "has_keyword_urgent         float64\n",
            "is_weekend_deadline        float64\n",
            "category_encoded           float64\n",
            "status_encoded             float64\n",
            "priority_encoded           float64\n",
            "is_completed               float64\n",
            "user_current_load          float64\n",
            "past_behavior_score        float64\n",
            "workload                   float64\n",
            "deadline_days              float64\n",
            "dtype: object\n",
            "\n",
            "Missing values per column:\n",
            "task_id                      0\n",
            "task_description             0\n",
            "deadline                     0\n",
            "priority                     0\n",
            "assigned_user                0\n",
            "user_workload                0\n",
            "past_task_behavior           0\n",
            "category                     0\n",
            "status                       0\n",
            "time_taken_(hours)           0\n",
            "deadline_days_remaining    186\n",
            "task_length                186\n",
            "has_keyword_urgent         186\n",
            "is_weekend_deadline        186\n",
            "category_encoded           186\n",
            "status_encoded             186\n",
            "priority_encoded           186\n",
            "is_completed               186\n",
            "user_current_load          186\n",
            "past_behavior_score        186\n",
            "workload                   186\n",
            "deadline_days              186\n",
            "dtype: int64\n",
            "\n",
            "Unique values per column:\n",
            "task_id: 390\n",
            "task_description: 37\n",
            "deadline: 58\n",
            "priority: 4\n",
            "assigned_user: 20\n",
            "user_workload: 10\n",
            "past_task_behavior: 7\n",
            "category: 6\n",
            "status: 4\n",
            "time_taken_(hours): 158\n",
            "deadline_days_remaining: 58\n",
            "task_length: 4\n",
            "has_keyword_urgent: 2\n",
            "is_weekend_deadline: 2\n",
            "category_encoded: 6\n",
            "status_encoded: 3\n",
            "priority_encoded: 3\n",
            "is_completed: 2\n",
            "user_current_load: 8\n",
            "past_behavior_score: 9\n",
            "workload: 256\n",
            "deadline_days: 58\n",
            "\n",
            "Distribution for priority:\n",
            "priority\n",
            "High      127\n",
            "Urgent    120\n",
            "Medium    120\n",
            "Low       119\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution for category:\n",
            "category\n",
            "Design           91\n",
            "Documentation    87\n",
            "DevOps           83\n",
            "Testing          80\n",
            "Development      74\n",
            "Management       71\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution for assigned_user:\n",
            "assigned_user\n",
            "User_1     36\n",
            "user7      35\n",
            "User_6     34\n",
            "User_7     34\n",
            "User_5     34\n",
            "User_8     31\n",
            "User_10    29\n",
            "user8      27\n",
            "User_2     27\n",
            "User_9     26\n",
            "User_4     25\n",
            "User_3     24\n",
            "user2      19\n",
            "user1      19\n",
            "user9      18\n",
            "user10     18\n",
            "user5      17\n",
            "user4      12\n",
            "user3      11\n",
            "user6      10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution for status:\n",
            "status\n",
            "To Do          186\n",
            "Completed      120\n",
            "In Progress     96\n",
            "Pending         84\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution for past_task_behavior:\n",
            "past_task_behavior\n",
            "delayed              110\n",
            "on-time              106\n",
            "late                  84\n",
            "On Time               55\n",
            "Needs Improvement     53\n",
            "Late                  39\n",
            "Excellent             39\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Duplicate rows: 96\n",
            "\n",
            "Numeric Columns Summary:\n",
            "       user_workload  time_taken_(hours)  deadline_days_remaining  \\\n",
            "count     486.000000          486.000000               300.000000   \n",
            "mean        5.323045            4.557904                29.910000   \n",
            "std         2.954040            4.218288                17.402626   \n",
            "min         1.000000            0.000000                 1.000000   \n",
            "25%         2.000000            0.000000                15.000000   \n",
            "50%         5.000000            4.523914                29.500000   \n",
            "75%         8.000000            8.602229                44.000000   \n",
            "max        10.000000           12.000000                59.000000   \n",
            "\n",
            "       task_length  has_keyword_urgent  is_weekend_deadline  category_encoded  \\\n",
            "count   300.000000          300.000000           300.000000        300.000000   \n",
            "mean      4.666667            0.053333             0.306667          2.396667   \n",
            "std       0.930111            0.225073             0.461880          1.695752   \n",
            "min       3.000000            0.000000             0.000000          0.000000   \n",
            "25%       4.000000            0.000000             0.000000          1.000000   \n",
            "50%       5.000000            0.000000             0.000000          2.000000   \n",
            "75%       5.000000            0.000000             1.000000          4.000000   \n",
            "max       6.000000            1.000000             1.000000          5.000000   \n",
            "\n",
            "       status_encoded  priority_encoded  is_completed  user_current_load  \\\n",
            "count      300.000000        300.000000    300.000000         300.000000   \n",
            "mean         0.880000          1.000000      0.400000          18.406667   \n",
            "std          0.817206          0.817861      0.490716           3.482583   \n",
            "min          0.000000          0.000000      0.000000          13.000000   \n",
            "25%          0.000000          0.000000      0.000000          15.000000   \n",
            "50%          1.000000          1.000000      0.000000          18.000000   \n",
            "75%          2.000000          2.000000      1.000000          20.000000   \n",
            "max          2.000000          2.000000      1.000000          25.000000   \n",
            "\n",
            "       past_behavior_score    workload  deadline_days  \n",
            "count           300.000000  300.000000     300.000000  \n",
            "mean              0.353333    1.541782      29.910000  \n",
            "std               0.060934    0.548017      17.402626  \n",
            "min               0.250000    0.051724       1.000000  \n",
            "25%               0.309682    1.131705      15.000000  \n",
            "50%               0.382353    1.564176      29.500000  \n",
            "75%               0.387097    1.940134      44.000000  \n",
            "max               0.481481    2.948276      59.000000  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"final_task_dataset_balanced.csv\")\n",
        "\n",
        "# ===== Basic Info =====\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# ===== Missing Values =====\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# ===== Unique Values =====\n",
        "print(\"\\nUnique values per column:\")\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].nunique()}\")\n",
        "\n",
        "# ===== Categorical Value Distribution =====\n",
        "categorical_cols = ['priority', 'category', 'assigned_user', 'status', 'past_task_behavior']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        print(f\"\\nDistribution for {col}:\")\n",
        "        print(df[col].value_counts())\n",
        "\n",
        "# ===== Duplicate Check =====\n",
        "print(\"\\nDuplicate rows:\", df.duplicated().sum())\n",
        "\n",
        "# ===== Numeric Stats =====\n",
        "print(\"\\nNumeric Columns Summary:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ===== Load dataset =====\n",
        "df = pd.read_csv(\"final_task_dataset_balanced.csv\")\n",
        "\n",
        "# ===== Standardize categorical values =====\n",
        "# Fix past_task_behavior inconsistencies\n",
        "behavior_map = {\n",
        "    \"On Time\": \"On Time\",\n",
        "    \"on-time\": \"On Time\",\n",
        "    \"Late\": \"Late\",\n",
        "    \"late\": \"Late\",\n",
        "    \"Needs Improvement\": \"Needs Improvement\",\n",
        "    \"delayed\": \"Delayed\",\n",
        "    \"Delayed\": \"Delayed\",\n",
        "    \"Excellent\": \"Excellent\"\n",
        "}\n",
        "df['past_task_behavior'] = df['past_task_behavior'].map(behavior_map)\n",
        "\n",
        "# ===== Convert deadline to datetime =====\n",
        "df['deadline'] = pd.to_datetime(df['deadline'], errors='coerce')\n",
        "\n",
        "# ===== Recompute derived columns =====\n",
        "today = pd.Timestamp.today()\n",
        "df['deadline_days_remaining'] = (df['deadline'] - today).dt.days.clip(lower=0)\n",
        "df['deadline_days'] = df['deadline_days_remaining']\n",
        "df['task_length'] = df['task_description'].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
        "df['has_keyword_urgent'] = df['task_description'].fillna(\"\").apply(lambda x: 1 if 'urgent' in str(x).lower() else 0)\n",
        "df['is_weekend_deadline'] = df['deadline'].dt.weekday.apply(lambda x: 1 if x in [5, 6] else 0)\n",
        "\n",
        "# ===== Map past_task_behavior to numeric score =====\n",
        "behavior_score_map = {\n",
        "    \"On Time\": 1.0,\n",
        "    \"Late\": 0.5,\n",
        "    \"Needs Improvement\": 0.3,\n",
        "    \"Delayed\": 0.3,\n",
        "    \"Excellent\": 1.2\n",
        "}\n",
        "df['past_behavior_score'] = df['past_task_behavior'].map(behavior_score_map).fillna(0.5)\n",
        "\n",
        "# ===== Impute missing values =====\n",
        "# Numeric columns → median\n",
        "numeric_cols = ['user_current_load', 'workload', 'time_taken_(hours)', 'deadline_days_remaining',\n",
        "                'task_length', 'has_keyword_urgent', 'is_weekend_deadline', 'category_encoded',\n",
        "                'status_encoded', 'priority_encoded', 'is_completed', 'past_behavior_score', 'deadline_days']\n",
        "for col in numeric_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Categorical columns → mode\n",
        "categorical_cols = ['priority', 'category', 'status', 'assigned_user']\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "# ===== Normalize numeric columns =====\n",
        "scaler = MinMaxScaler()\n",
        "df[['user_current_load', 'workload', 'time_taken_(hours)', 'deadline_days_remaining', 'task_length']] = \\\n",
        "    scaler.fit_transform(df[['user_current_load', 'workload', 'time_taken_(hours)', 'deadline_days_remaining', 'task_length']])\n",
        "\n",
        "# ===== Drop duplicates =====\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# ===== Save cleaned dataset =====\n",
        "df.to_csv(\"fully_cleaned_task_dataset.csv\", index=False)\n",
        "\n",
        "# ===== Check balance =====\n",
        "print(\"✅ Cleaning Complete!\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nPriority distribution:\\n\", df['priority'].value_counts())\n",
        "print(\"\\nCategory distribution:\\n\", df['category'].value_counts())\n",
        "print(\"\\nStatus distribution:\\n\", df['status'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46-UGt2PcHlE",
        "outputId": "f597319e-cb20-4d5e-a3e6-c49471bde222"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaning Complete!\n",
            "Dataset shape: (390, 22)\n",
            "\n",
            "Priority distribution:\n",
            " priority\n",
            "High      127\n",
            "Medium    120\n",
            "Low       119\n",
            "Urgent     24\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Category distribution:\n",
            " category\n",
            "DevOps           65\n",
            "Development      65\n",
            "Design           65\n",
            "Documentation    65\n",
            "Testing          65\n",
            "Management       65\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Status distribution:\n",
            " status\n",
            "Completed      120\n",
            "In Progress     96\n",
            "To Do           90\n",
            "Pending         84\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk, re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"fully_cleaned_task_dataset.csv\")\n",
        "\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing function\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()  # lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation/numbers\n",
        "    tokens = nltk.word_tokenize(text)  # tokenize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # remove stopwords & lemmatize\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply NLP cleaning\n",
        "df['task_description_clean'] = df['task_description'].apply(clean_text)\n",
        "\n",
        "# Save updated dataset\n",
        "df.to_csv(\"nlp_cleaned_task_dataset.csv\", index=False)\n",
        "\n",
        "print(\"✅ NLP preprocessing complete! Added column 'task_description_clean'.\")\n",
        "print(df[['task_description', 'task_description_clean']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPKTh6HtesR3",
        "outputId": "7b93f4d9-6916-4eb7-f990-30166c0a514d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ NLP preprocessing complete! Added column 'task_description_clean'.\n",
            "                           task_description  \\\n",
            "0  Setup monitoring for production servers.   \n",
            "1              Restore database from backup   \n",
            "2           Implement search functionality.   \n",
            "3      Update color palette and typography.   \n",
            "4        Write API reference documentation.   \n",
            "\n",
            "               task_description_clean  \n",
            "0  setup monitoring production server  \n",
            "1             restore database backup  \n",
            "2      implement search functionality  \n",
            "3     update color palette typography  \n",
            "4   write api reference documentation  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# === Load dataset ===\n",
        "df.to_csv(\"nlp_cleaned_task_dataset.csv\")\n",
        "\n",
        "# === Features & Labels ===\n",
        "X = df['task_description_clean']\n",
        "y = df['category']\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# === Train-Test Split ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# === TF-IDF (optimized for size) ===\n",
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2), stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# === Base Models ===\n",
        "nb = MultinomialNB()\n",
        "rf = RandomForestClassifier(n_estimators=200, max_depth=25, random_state=42)\n",
        "xgb = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# === Voting Ensemble ===\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('nb', nb), ('rf', rf), ('xgb', xgb)],\n",
        "    voting='soft',  # soft = uses predicted probabilities\n",
        "    weights=[1,2,3]  # give higher weight to xgb\n",
        ")\n",
        "\n",
        "# === Train ===\n",
        "voting_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# === Evaluate ===\n",
        "y_pred = voting_clf.predict(X_test_tfidf)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Voting Ensemble Accuracy: {acc:.4f}\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# === Save Models & Vectorizer ===\n",
        "joblib.dump(voting_clf, \"voting_ensemble_task_classifier.joblib\", compress=3)\n",
        "joblib.dump(vectorizer, \"task_tfidf_vectorizer.joblib\", compress=3)\n",
        "joblib.dump(label_encoder, \"task_label_encoder.joblib\", compress=3)\n",
        "\n",
        "print(\"✅ Ensemble model, TF-IDF vectorizer, and label encoder saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R43Bsvg8VUE5",
        "outputId": "bccf93d9-2cf1-4bdc-c7ce-0ceadc4cf19e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Ensemble Accuracy: 0.4103\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Design       0.33      0.23      0.27        13\n",
            "       DevOps       0.32      0.46      0.38        13\n",
            "  Development       0.30      0.23      0.26        13\n",
            "Documentation       0.58      0.54      0.56        13\n",
            "   Management       0.62      0.38      0.48        13\n",
            "      Testing       0.40      0.62      0.48        13\n",
            "\n",
            "     accuracy                           0.41        78\n",
            "    macro avg       0.43      0.41      0.40        78\n",
            " weighted avg       0.43      0.41      0.40        78\n",
            "\n",
            "✅ Ensemble model, TF-IDF vectorizer, and label encoder saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rjsurz9ue9U9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# === Load preprocessed dataset ===\n",
        "df = pd.read_csv(\"nlp_cleaned_task_dataset.csv\")  # use cleaned + NLP dataset\n",
        "X = df['task_description_clean']\n",
        "y = df['priority']\n",
        "\n",
        "# === Encode priority labels ===\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# === Train/Test Split ===\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# === Compute Class Weights ===\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=label_encoder.classes_, y=y)\n",
        "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
        "\n",
        "# === TF-IDF Vectorizer (size tuned for XGBoost) ===\n",
        "tfidf_xgb = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
        "X_train_vec = tfidf_xgb.fit_transform(X_train_raw)\n",
        "X_test_vec = tfidf_xgb.transform(X_test_raw)\n",
        "\n",
        "# === XGBoost Model (lightweight params) ===\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=2,  # Helps balance\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# === Train ===\n",
        "xgb_model.fit(X_train_vec, y_train)\n",
        "\n",
        "# === Evaluate ===\n",
        "y_pred = xgb_model.predict(X_test_vec)\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# === Save model, vectorizer, and encoder ===\n",
        "joblib.dump(xgb_model, \"priority_xgboost.pkl\")\n",
        "joblib.dump(tfidf_xgb, \"priority_tfidf_vectorizer.pkl\")\n",
        "joblib.dump(label_encoder, \"priority_label_encoder.pkl\")\n",
        "\n",
        "print(\"\\n✅ XGBoost model, TF-IDF vectorizer, and label encoder saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dT4j48vfmbpk",
        "outputId": "65ff52e0-b278-4f3b-dd28-a52f031bf224"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:07:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Accuracy: 0.8461538461538461\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        High       0.79      0.92      0.85        25\n",
            "         Low       0.88      0.96      0.92        24\n",
            "      Medium       1.00      0.75      0.86        24\n",
            "      Urgent       0.40      0.40      0.40         5\n",
            "\n",
            "    accuracy                           0.85        78\n",
            "   macro avg       0.77      0.76      0.76        78\n",
            "weighted avg       0.86      0.85      0.85        78\n",
            "\n",
            "\n",
            "✅ XGBoost model, TF-IDF vectorizer, and label encoder saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# === Load cleaned dataset ===\n",
        "df = pd.read_csv(\"nlp_cleaned_task_dataset.csv\")\n",
        "\n",
        "# === 1. Unify usernames ===\n",
        "df['assigned_user'] = df['assigned_user'].str.lower().str.replace(' ', '').str.replace('-', '_')\n",
        "df['assigned_user'] = df['assigned_user'].apply(lambda x: x if x.startswith('user_') else x.replace('user', 'user_'))\n",
        "\n",
        "# === 2. Group rare users ===\n",
        "user_counts = df['assigned_user'].value_counts()\n",
        "rare_users = user_counts[user_counts < 5].index\n",
        "df['assigned_user'] = df['assigned_user'].apply(lambda x: 'other' if x in rare_users else x)\n",
        "\n",
        "print(\"New user distribution:\\n\", df['assigned_user'].value_counts())\n",
        "\n",
        "# === 3. Encode target ===\n",
        "le = LabelEncoder()\n",
        "df['assigned_user_encoded'] = le.fit_transform(df['assigned_user'])\n",
        "\n",
        "# === Features ===\n",
        "text_features = df['task_description_clean']\n",
        "numeric_features = df[['category_encoded', 'priority_encoded', 'deadline_days',\n",
        "                       'has_keyword_urgent', 'task_length', 'user_current_load',\n",
        "                       'user_workload', 'past_behavior_score', 'is_weekend_deadline']]\n",
        "\n",
        "# Save the order of numeric feature names for dashboard consistency\n",
        "feature_names = list(numeric_features.columns)\n",
        "joblib.dump(feature_names, \"user_assignment_feature_names.pkl\")\n",
        "\n",
        "# === 4. TF-IDF Vectorization ===\n",
        "tfidf = TfidfVectorizer(max_features=300)\n",
        "X_text = tfidf.fit_transform(text_features)\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(numeric_features)\n",
        "\n",
        "X = hstack([X_text, X_num])\n",
        "y = df['assigned_user_encoded']\n",
        "\n",
        "# === 5. Train/Test Split ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# === 6. SMOTE Oversampling ===\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# === 7. Compute class weights ===\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_bal), y=y_train_bal)\n",
        "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
        "\n",
        "# === 8. Train XGBoost ===\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    scale_pos_weight=1  # XGB auto-balances, but we oversampled too\n",
        ")\n",
        "xgb_model.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# === 9. Evaluate ===\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "print(\"\\nXGBoost Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "# === 10. Save everything ===\n",
        "joblib.dump(xgb_model, \"user_assignment_xgb.pkl\")\n",
        "joblib.dump(tfidf, \"user_assignment_tfidf.pkl\")\n",
        "joblib.dump(le, \"user_assignment_label_encoder.pkl\")\n",
        "joblib.dump(scaler, \"user_assignment_scaler.pkl\")\n",
        "joblib.dump(feature_names, \"user_assignment_feature_names.pkl\")  # <--- ADDED\n",
        "\n",
        "print(\"✅ User assignment model, TF-IDF, label encoder, scaler, and feature names saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMIvaGHmMs2h",
        "outputId": "cfc91075-3bc7-4bea-f10b-23c44b0d8a72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New user distribution:\n",
            " assigned_user\n",
            "user_7     49\n",
            "user_1     45\n",
            "user_6     41\n",
            "user_8     41\n",
            "user_10    41\n",
            "user_5     40\n",
            "user_2     36\n",
            "user_9     36\n",
            "user_4     33\n",
            "user_3     28\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:07:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"scale_pos_weight\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost Accuracy: 0.6153846153846154\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      user_1       1.00      0.78      0.88         9\n",
            "     user_10       0.00      0.00      0.00         8\n",
            "      user_2       0.60      0.86      0.71         7\n",
            "      user_3       0.83      0.83      0.83         6\n",
            "      user_4       0.86      0.86      0.86         7\n",
            "      user_5       0.50      0.38      0.43         8\n",
            "      user_6       0.78      0.88      0.82         8\n",
            "      user_7       0.50      0.50      0.50        10\n",
            "      user_8       0.70      0.88      0.78         8\n",
            "      user_9       0.22      0.29      0.25         7\n",
            "\n",
            "    accuracy                           0.62        78\n",
            "   macro avg       0.60      0.62      0.61        78\n",
            "weighted avg       0.60      0.62      0.60        78\n",
            "\n",
            "✅ User assignment model, TF-IDF, label encoder, scaler, and feature names saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from datetime import datetime, date\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# === Load all models and encoders ===\n",
        "category_model = joblib.load(\"voting_ensemble_task_classifier.joblib\")\n",
        "category_vectorizer = joblib.load(\"task_tfidf_vectorizer.joblib\")\n",
        "category_label_encoder = joblib.load(\"task_label_encoder.joblib\")\n",
        "\n",
        "priority_model = joblib.load(\"priority_xgboost.pkl\")\n",
        "priority_vectorizer = joblib.load(\"priority_tfidf_vectorizer.pkl\")\n",
        "priority_label_encoder = joblib.load(\"priority_label_encoder.pkl\")\n",
        "\n",
        "user_model = joblib.load(\"user_assignment_xgb.pkl\")\n",
        "user_vectorizer = joblib.load(\"user_assignment_tfidf.pkl\")\n",
        "user_label_encoder = joblib.load(\"user_assignment_label_encoder.pkl\")\n",
        "user_scaler = joblib.load(\"user_assignment_scaler.pkl\")\n",
        "user_feature_names = joblib.load(\"user_assignment_feature_names.pkl\")\n",
        "\n",
        "# === Load dataset (for dropdown) ===\n",
        "df = pd.read_csv(\"nlp_cleaned_task_dataset.csv\")\n",
        "\n",
        "# === Helper function: Days left ===\n",
        "def calculate_days_left(deadline_str):\n",
        "    try:\n",
        "        deadline = datetime.strptime(deadline_str, \"%Y-%m-%d\")\n",
        "        return max((deadline - datetime.now()).days, 0)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# === Prediction function ===\n",
        "def predict_all(task_description, deadline):\n",
        "    # --- Category prediction ---\n",
        "    X_cat = category_vectorizer.transform([task_description])\n",
        "    category_pred = category_model.predict(X_cat)[0]\n",
        "    category_name = category_label_encoder.inverse_transform([category_pred])[0]\n",
        "\n",
        "    # --- Priority prediction ---\n",
        "    X_pri = priority_vectorizer.transform([task_description])\n",
        "    priority_pred = priority_model.predict(X_pri)[0]\n",
        "    priority_name = priority_label_encoder.inverse_transform([priority_pred])[0]\n",
        "\n",
        "    # --- Base numeric features ---\n",
        "    sample_row = df.sample(1).iloc[0]  # For filling context-dependent features\n",
        "    base_numeric = {\n",
        "        'category_encoded': category_pred,\n",
        "        'priority_encoded': priority_pred,\n",
        "        'deadline_days': calculate_days_left(deadline),\n",
        "        'has_keyword_urgent': int(\"urgent\" in task_description.lower()),\n",
        "        'task_length': len(task_description.split()),\n",
        "        'user_current_load': sample_row['user_current_load'],\n",
        "        'user_workload': sample_row['user_workload'],\n",
        "        'past_behavior_score': sample_row['past_behavior_score'],\n",
        "        'is_weekend_deadline': 1 if datetime.strptime(deadline, \"%Y-%m-%d\").weekday() >= 5 else 0\n",
        "    }\n",
        "    numeric_features = pd.DataFrame([base_numeric])\n",
        "\n",
        "    # --- Ensure all training columns exist ---\n",
        "    for col in user_feature_names:\n",
        "        if col not in numeric_features.columns:\n",
        "            numeric_features[col] = 0  # Fill missing features with 0\n",
        "\n",
        "    numeric_features = numeric_features[user_feature_names]  # Reorder\n",
        "\n",
        "    # --- Scale numeric ---\n",
        "    numeric_scaled = user_scaler.transform(numeric_features)\n",
        "\n",
        "    # --- Combine text + numeric ---\n",
        "    X_text_user = user_vectorizer.transform([task_description])\n",
        "    X_user_final = hstack([X_text_user, numeric_scaled])\n",
        "\n",
        "    # --- User prediction ---\n",
        "    user_pred = user_model.predict(X_user_final)[0]\n",
        "    assigned_user = user_label_encoder.inverse_transform([user_pred])[0]\n",
        "\n",
        "    # --- Days left ---\n",
        "    days_left = calculate_days_left(deadline)\n",
        "\n",
        "    return category_name, priority_name, assigned_user, days_left\n",
        "\n",
        "# === Streamlit UI ===\n",
        "st.set_page_config(page_title=\"AI Task Management System\", layout=\"wide\")\n",
        "st.title(\"AI-Powered Task Management System\")\n",
        "\n",
        "# --- Task input ---\n",
        "task_description = st.selectbox(\"Select a Task from Dataset\", df['task_description_clean'].unique())\n",
        "\n",
        "# Deadline should start from today\n",
        "deadline = st.date_input(\"Deadline (YYYY-MM-DD)\", min_value=date.today())\n",
        "deadline_str = deadline.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# --- Predict button ---\n",
        "if st.button(\"Assign Task\"):\n",
        "    category_name, priority_name, assigned_user, days_left = predict_all(task_description, deadline_str)\n",
        "\n",
        "    st.subheader(\"Prediction Results:\")\n",
        "    st.write(f\"**Task Category:** {category_name}\")\n",
        "    st.write(f\"**Task Priority:** {priority_name}\")\n",
        "    st.write(f\"**Assigned User:** {assigned_user}\")\n",
        "    st.write(f\"**Days Left until Deadline:** {days_left}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_Mi2SKVe5lz",
        "outputId": "bc74ea5d-51c6-49f5-e721-58478d230071"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-24 16:08:55.317 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.320 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.453 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-07-24 16:08:55.457 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.460 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.462 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.463 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.467 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.468 Session state does not function when running a script without `streamlit run`\n",
            "2025-07-24 16:08:55.469 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.470 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.470 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.472 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.472 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.473 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.474 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.475 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.476 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.476 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.479 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.481 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-24 16:08:55.481 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dashboard_code = \"\"\"\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from datetime import datetime, date\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# === Load all models and encoders ===\n",
        "category_model = joblib.load(\"voting_ensemble_task_classifier.joblib\")\n",
        "category_vectorizer = joblib.load(\"task_tfidf_vectorizer.joblib\")\n",
        "category_label_encoder = joblib.load(\"task_label_encoder.joblib\")\n",
        "\n",
        "priority_model = joblib.load(\"priority_xgboost.pkl\")\n",
        "priority_vectorizer = joblib.load(\"priority_tfidf_vectorizer.pkl\")\n",
        "priority_label_encoder = joblib.load(\"priority_label_encoder.pkl\")\n",
        "\n",
        "user_model = joblib.load(\"user_assignment_xgb.pkl\")\n",
        "user_vectorizer = joblib.load(\"user_assignment_tfidf.pkl\")\n",
        "user_label_encoder = joblib.load(\"user_assignment_label_encoder.pkl\")\n",
        "user_scaler = joblib.load(\"user_assignment_scaler (1).pkl\")\n",
        "user_feature_names = joblib.load(\"user_assignment_feature_names.pkl\")\n",
        "\n",
        "# === Load dataset (for dropdown) ===\n",
        "df = pd.read_csv(\"nlp_cleaned_task_dataset.csv\")\n",
        "\n",
        "# === Helper function: Days left ===\n",
        "def calculate_days_left(deadline_str):\n",
        "    try:\n",
        "        deadline = datetime.strptime(deadline_str, \"%Y-%m-%d\")\n",
        "        return max((deadline - datetime.now()).days, 0)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# === Prediction function ===\n",
        "def predict_all(task_description, deadline):\n",
        "    # --- Category prediction ---\n",
        "    X_cat = category_vectorizer.transform([task_description])\n",
        "    category_pred = category_model.predict(X_cat)[0]\n",
        "    category_name = category_label_encoder.inverse_transform([category_pred])[0]\n",
        "\n",
        "    # --- Priority prediction ---\n",
        "    X_pri = priority_vectorizer.transform([task_description])\n",
        "    priority_pred = priority_model.predict(X_pri)[0]\n",
        "    priority_name = priority_label_encoder.inverse_transform([priority_pred])[0]\n",
        "\n",
        "    # --- Base numeric features ---\n",
        "    sample_row = df.sample(1).iloc[0]  # For filling context-dependent features\n",
        "    base_numeric = {\n",
        "        'category_encoded': category_pred,\n",
        "        'priority_encoded': priority_pred,\n",
        "        'deadline_days': calculate_days_left(deadline),\n",
        "        'has_keyword_urgent': int(\"urgent\" in task_description.lower()),\n",
        "        'task_length': len(task_description.split()),\n",
        "        'user_current_load': sample_row['user_current_load'],\n",
        "        'user_workload': sample_row['user_workload'],\n",
        "        'past_behavior_score': sample_row['past_behavior_score'],\n",
        "        'is_weekend_deadline': 1 if datetime.strptime(deadline, \"%Y-%m-%d\").weekday() >= 5 else 0\n",
        "    }\n",
        "    numeric_features = pd.DataFrame([base_numeric])\n",
        "\n",
        "    # --- Ensure all training columns exist ---\n",
        "    for col in user_feature_names:\n",
        "        if col not in numeric_features.columns:\n",
        "            numeric_features[col] = 0  # Fill missing features with 0\n",
        "\n",
        "    numeric_features = numeric_features[user_feature_names]  # Reorder\n",
        "\n",
        "    # --- Scale numeric ---\n",
        "    numeric_scaled = user_scaler.transform(numeric_features)\n",
        "\n",
        "    # --- Combine text + numeric ---\n",
        "    X_text_user = user_vectorizer.transform([task_description])\n",
        "    X_user_final = hstack([X_text_user, numeric_scaled])\n",
        "\n",
        "    # --- User prediction ---\n",
        "    user_pred = user_model.predict(X_user_final)[0]\n",
        "    assigned_user = user_label_encoder.inverse_transform([user_pred])[0]\n",
        "\n",
        "    # --- Days left ---\n",
        "    days_left = calculate_days_left(deadline)\n",
        "\n",
        "    return category_name, priority_name, assigned_user, days_left\n",
        "\n",
        "# === Streamlit UI ===\n",
        "st.set_page_config(page_title=\"AI Task Management System\", layout=\"wide\")\n",
        "st.title(\"AI-Powered Task Management System\")\n",
        "\n",
        "# --- Task input ---\n",
        "task_description = st.selectbox(\"Select a Task from Dataset\", df['task_description_clean'].unique())\n",
        "\n",
        "# Deadline should start from today\n",
        "deadline = st.date_input(\"Deadline (YYYY-MM-DD)\", min_value=date.today())\n",
        "deadline_str = deadline.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# --- Predict button ---\n",
        "if st.button(\"Assign Task\"):\n",
        "    category_name, priority_name, assigned_user, days_left = predict_all(task_description, deadline_str)\n",
        "\n",
        "    st.subheader(\"Prediction Results:\")\n",
        "    st.write(f\"**Task Category:** {category_name}\")\n",
        "    st.write(f\"**Task Priority:** {priority_name}\")\n",
        "    st.write(f\"**Assigned User:** {assigned_user}\")\n",
        "    st.write(f\"**Days Left until Deadline:** {days_left}\")\n",
        "\"\"\"\n",
        "\n",
        "# Save to file\n",
        "with open(\"dashboard.py\", \"w\") as f:\n",
        "    f.write(dashboard_code)\n",
        "\n",
        "print(\"✅ Dashboard file saved as dashboard.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADQR4RQVivth",
        "outputId": "ca02e34d-c0be-4a77-e89c-a4032fa2f8e5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dashboard file saved as dashboard.py\n"
          ]
        }
      ]
    }
  ]
}